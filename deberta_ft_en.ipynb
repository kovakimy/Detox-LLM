{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88bd57f-afb3-40df-916e-65c0a779ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d59ea-e425-412e-a7a6-7a2c1dc420f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997d1d82-2737-43d3-904f-fd409ddce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NORMAL\", 1: \"TOXIC\"}\n",
    "label2id = {\"NORMAL\": 0, \"TOXIC\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e080d4-c36e-42d3-9783-fdee330f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-xlarge-mnli\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-xlarge-mnli\", num_labels=2, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-xlarge-mnli\")\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c43e01-b292-4913-a926-54c6ef8c5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NORMAL', '1': 'TOXIC'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NORMAL', '1': 'TOXIC'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NORMAL', '1': 'TOXIC'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/sslashinin/kovakimyan/diploma/deberta-xlarge-mnli/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6bb5ec5-3a09-401c-a92a-0aa69579d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NORMAL', '1': 'TOXIC'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/sslashinin/kovakimyan/diploma/deberta-xlarge-mnli/ and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/sslashinin/kovakimyan/diploma/deberta-xlarge-mnli/\", local_files_only=True, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea43f7d7-2fce-4677-a3f4-521b538e2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/sslashinin/.cache/huggingface/datasets/textdetox___parquet/textdetox--multilingual_toxicity_dataset-c4d45e458288ed14/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85330ff7333242879aff78f0110cea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    am: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    ar: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    de: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    en: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    es: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    hi: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    ru: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    uk: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    zh: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"textdetox/multilingual_toxicity_dataset\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2004ba0e-6073-413f-92aa-2cebc28c0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'toxic'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14da260-01c6-4f47-a6bd-9b75aee850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def clear_sentance(sentance):\n",
    "    sentance= re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = cleanPunc(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in  stopwords.words('english'))\n",
    "    return sentance.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42726fa7-3dd9-433e-bc14-fee9e2cb810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/tmp/job-1888621/ipykernel_223617/3189064709.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 523.68it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = []\n",
    "for sentance in tqdm(dataset[\"en\"]['text']):\n",
    "    preprocessed_text.append(clear_sentance(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1df45d47-0ea4-42e5-ae18-4ff85f025331",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = copy.deepcopy(dataset[\"en\"])\n",
    "\n",
    "new_dataset = new_dataset.add_column(\"preprocessed_text\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc3a14ef-2c99-4e95-a57e-8f3edac718f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset = copy.deepcopy(new_dataset)\n",
    "\n",
    "en_dataset_split = en_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "en_dataset_split = en_dataset_split.rename_column(\"toxic\", \"label\")\n",
    "\n",
    "en_dataset_split[\"train\"] = en_dataset_split[\"train\"].remove_columns(\"text\")\n",
    "en_dataset_split[\"test\"] = en_dataset_split[\"test\"].remove_columns(\"text\")\n",
    "\n",
    "en_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e19aa4e-3aa9-4952-a57b-2466bcea6e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2f8319e30e4aeeb810447e776a56d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d0bfa351a47398b33194976e5b0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"preprocessed_text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = en_dataset_split.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10570a80-a836-40ad-84ad-d6c6ff7d9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3cf2662-6450-4be1-bbfa-e547505039d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "701f2416-9ee5-4451-b7aa-213187aeeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40a5c49c-c688-4428-b91b-5ec077ef9119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 20:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219914</td>\n",
       "      <td>0.949333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.246344</td>\n",
       "      <td>0.941333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.206414</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.220114</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.237834</td>\n",
       "      <td>0.957333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta_xlarge_mnli_ft\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ca17654-962d-45b1-bdf4-35fbed3f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bace6aeb-d322-4554-a8d1-4bfc8beebffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    print(model.config.id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51d37e72-f67d-4f46-a5b0-b8749e421a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOXIC\n",
      "TOXIC\n",
      "NORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "text = \"Fuck you\"\n",
    "run_model(text)\n",
    "\n",
    "text = \"You act like a fool\"\n",
    "run_model(text)\n",
    "\n",
    "text = \"Today is sunny\"\n",
    "run_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32df74-a207-490b-a3bd-99899e368dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-kovakimyan]",
   "language": "python",
   "name": "conda-env-.conda-kovakimyan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
