{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88bd57f-afb3-40df-916e-65c0a779ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8661aa59-44fb-457e-8d58-b60fa9edc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 17 20:26:54 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              78W / 300W |  19467MiB / 32768MiB |     26%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              81W / 300W |  17877MiB / 32768MiB |     23%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     17809      C   ...conda/envs/llama_factory/bin/python    15242MiB |\n",
      "|    0   N/A  N/A    274087      C   ...conda/envs/llama_factory/bin/python     4222MiB |\n",
      "|    1   N/A  N/A     17809      C   ...conda/envs/llama_factory/bin/python    15066MiB |\n",
      "|    1   N/A  N/A    274087      C   ...conda/envs/llama_factory/bin/python     2808MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d59ea-e425-412e-a7a6-7a2c1dc420f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sslashinin/.conda/envs/llama_factory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997d1d82-2737-43d3-904f-fd409ddce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Not_hate\", 1: \"Hate\"}\n",
    "label2id = {\"Not_hate\": 0, \"Hate\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e080d4-c36e-42d3-9783-fdee330f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-xlarge-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-xlarge-mnli\", num_labels=2, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea43f7d7-2fce-4677-a3f4-521b538e2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"/home/sslashinin/kovakimyan/diploma/dataset/super_toxic/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14da260-01c6-4f47-a6bd-9b75aee850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def clear_sentance(sentance):\n",
    "    sentance= re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = cleanPunc(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in  stopwords.words('english'))\n",
    "    return sentance.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42726fa7-3dd9-433e-bc14-fee9e2cb810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/5000 [00:00<01:40, 49.68it/s]/tmp/job-1989456/ipykernel_174232/3189064709.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
      "100%|██████████| 5000/5000 [00:54<00:00, 92.44it/s] \n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = []\n",
    "for sentance in tqdm(dataset['text']):\n",
    "    preprocessed_text.append(clear_sentance(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df45d47-0ea4-42e5-ae18-4ff85f025331",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = copy.deepcopy(dataset)\n",
    "\n",
    "new_dataset = new_dataset.add_column(\"preprocessed_text\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3a14ef-2c99-4e95-a57e-8f3edac718f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset = copy.deepcopy(new_dataset)\n",
    "\n",
    "en_dataset_split = en_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "en_dataset_split[\"train\"] = en_dataset_split[\"train\"].remove_columns(\"text\")\n",
    "en_dataset_split[\"test\"] = en_dataset_split[\"test\"].remove_columns(\"text\")\n",
    "\n",
    "en_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e19aa4e-3aa9-4952-a57b-2466bcea6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3500/3500 [00:00<00:00, 3698.12 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 5895.09 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"preprocessed_text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = en_dataset_split.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10570a80-a836-40ad-84ad-d6c6ff7d9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3cf2662-6450-4be1-bbfa-e547505039d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701f2416-9ee5-4451-b7aa-213187aeeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "901717bb-90e6-467d-aabe-60b89a201621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export WANDB_DISABLED=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf55893-17ba-4e57-aeef-b246b42f9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"hatespeech_detection_ft\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d56a7-92ac-42fa-8a82-54b272d88d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/home/sslashinin/kovakimyan/diploma/hatespeech_detection_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8564ab-07d8-4fef-beef-78c4a5ac555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/home/sslashinin/kovakimyan/diploma/hatespeech_detection_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca17654-962d-45b1-bdf4-35fbed3f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bace6aeb-d322-4554-a8d1-4bfc8beebffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    print(model.config.id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d37e72-f67d-4f46-a5b0-b8749e421a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate\n",
      "Hate\n",
      "Not_hate\n"
     ]
    }
   ],
   "source": [
    "text = \"Fuck you\"\n",
    "run_model(text)\n",
    "\n",
    "text = \"You act like a fool\"\n",
    "run_model(text)\n",
    "\n",
    "text = \"Today is sunny\"\n",
    "run_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32df74-a207-490b-a3bd-99899e368dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-llama_factory]",
   "language": "python",
   "name": "conda-env-.conda-llama_factory-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
