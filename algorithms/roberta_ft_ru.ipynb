{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88bd57f-afb3-40df-916e-65c0a779ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d59ea-e425-412e-a7a6-7a2c1dc420f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997d1d82-2737-43d3-904f-fd409ddce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NORMAL\", 1: \"TOXIC\"}\n",
    "label2id = {\"NORMAL\": 0, \"TOXIC\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e080d4-c36e-42d3-9783-fdee330f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-xlarge-mnli\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-xlarge-mnli\", num_labels=2, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-xlarge-mnli\")\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c43e01-b292-4913-a926-54c6ef8c5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/sslashinin/kovakimyan/diploma/xlm-roberta-base/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6bb5ec5-3a09-401c-a92a-0aa69579d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/sslashinin/kovakimyan/diploma/xlm-roberta-base/ were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /home/sslashinin/kovakimyan/diploma/xlm-roberta-base/ and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/sslashinin/kovakimyan/diploma/xlm-roberta-base/\", local_files_only=True, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea43f7d7-2fce-4677-a3f4-521b538e2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/sslashinin/.cache/huggingface/datasets/textdetox___parquet/textdetox--multilingual_toxicity_dataset-c4d45e458288ed14/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a34358e6134449ac3ed85b27a7ecb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    am: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    ar: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    de: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    en: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    es: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    hi: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    ru: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    uk: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    zh: Dataset({\n",
       "        features: ['text', 'toxic'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"textdetox/multilingual_toxicity_dataset\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2004ba0e-6073-413f-92aa-2cebc28c0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'toxic'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"ru\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14da260-01c6-4f47-a6bd-9b75aee850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def clear_sentance(sentance):\n",
    "    sentance= re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = cleanPunc(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in  stopwords.words('english'))\n",
    "    return sentance.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42726fa7-3dd9-433e-bc14-fee9e2cb810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/tmp/job-1888907/ipykernel_265194/3189064709.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2549.53it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = []\n",
    "for sentance in tqdm(dataset[\"ru\"]['text']):\n",
    "    preprocessed_text.append(clear_sentance(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df45d47-0ea4-42e5-ae18-4ff85f025331",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = copy.deepcopy(dataset[\"ru\"])\n",
    "\n",
    "new_dataset = new_dataset.add_column(\"preprocessed_text\", new_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3a14ef-2c99-4e95-a57e-8f3edac718f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset = copy.deepcopy(new_dataset)\n",
    "\n",
    "en_dataset_split = en_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "en_dataset_split = en_dataset_split.rename_column(\"toxic\", \"label\")\n",
    "\n",
    "en_dataset_split[\"train\"] = en_dataset_split[\"train\"].remove_columns(\"text\")\n",
    "en_dataset_split[\"test\"] = en_dataset_split[\"test\"].remove_columns(\"text\")\n",
    "\n",
    "en_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e19aa4e-3aa9-4952-a57b-2466bcea6e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521b7504de9248b1a029664df47f3a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d636e32aa6de4bb08a95206aec32f4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'preprocessed_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"preprocessed_text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = en_dataset_split.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10570a80-a836-40ad-84ad-d6c6ff7d9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3cf2662-6450-4be1-bbfa-e547505039d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701f2416-9ee5-4451-b7aa-213187aeeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a5c49c-c688-4428-b91b-5ec077ef9119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sslashinin/.conda/envs/kovakimyan/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33movakimyanchris\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sslashinin/kovakimyan/diploma/wandb/run-20240303_232454-e2tplbph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ovakimyanchris/huggingface/runs/e2tplbph' target=\"_blank\">drawn-darkness-8</a></strong> to <a href='https://wandb.ai/ovakimyanchris/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ovakimyanchris/huggingface' target=\"_blank\">https://wandb.ai/ovakimyanchris/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ovakimyanchris/huggingface/runs/e2tplbph' target=\"_blank\">https://wandb.ai/ovakimyanchris/huggingface/runs/e2tplbph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 04:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.952667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.241879</td>\n",
       "      <td>0.954667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.255611</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.228682</td>\n",
       "      <td>0.967333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.252315</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta_xlarge_mnli_ft_ru\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca17654-962d-45b1-bdf4-35fbed3f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f1084-02ab-47ec-b62e-a7c3a93fc67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bace6aeb-d322-4554-a8d1-4bfc8beebffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    print(model.config.id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d37e72-f67d-4f46-a5b0-b8749e421a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_1\n",
      "LABEL_0\n"
     ]
    }
   ],
   "source": [
    "text = \"Ведёшь себя как дурвк\"\n",
    "run_model(text)\n",
    "\n",
    "text = \"Сегодня солнечный день\"\n",
    "run_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32df74-a207-490b-a3bd-99899e368dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d567934a-a982-4a0b-8994-48918f37a48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'preprocessed_text'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset_split[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-kovakimyan]",
   "language": "python",
   "name": "conda-env-.conda-kovakimyan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
